# ğŸ•·ï¸ AICrawler - LangGraph TabanlÄ± Web Ä°Ã§erik Toplama ve Temizleme Sistemi

AICrawler, LangGraph kullanarak geliÅŸtirilmiÅŸ akÄ±llÄ± bir web iÃ§erik toplama (crawling) ve temizleme aracÄ±dÄ±r. Verilen bir baÅŸlangÄ±Ã§ URL'inden baÅŸlayarak, belirtilen derinlik ve sayfa limitlerine gÃ¶re web sitelerini tarar, HTML iÃ§eriÄŸini temizler ve Ã§eÅŸitli formatlarda export eder.

## ğŸŒŸ Ã–zellikler

- **LangGraph Workflow**: AdÄ±m bazlÄ± iÅŸ akÄ±ÅŸÄ± yÃ¶netimi (fetch â†’ clean â†’ save)
- **Ã‡ok KatmanlÄ± Tarama**: Derinlik (depth) ve sayfa limiti parametreleri ile kontrollÃ¼ tarama
- **AkÄ±llÄ± HTML Temizleme**: Script, style, reklam ve gereksiz elementlerin otomatik temizlenmesi
- **Ã‡oklu Export FormatlarÄ±**: JSON, Markdown ve PDF formatlarÄ±nda dÄ±ÅŸa aktarÄ±m
- **Modern Web ArayÃ¼zÃ¼**: Streamlit tabanlÄ± kullanÄ±cÄ± dostu arayÃ¼z
- **CanlÄ± Ä°lerleme Takibi**: Tarama sÄ±rasÄ±nda anlÄ±k durum gÃ¼ncellemeleri
- **Hata YÃ¶netimi**: BaÅŸarÄ±sÄ±z sayfalar iÃ§in detaylÄ± hata raporlama

## ğŸ“‹ Gereksinimler

- Python 3.8+
- Pip paket yÃ¶neticisi

## ğŸš€ Kurulum

1. Projeyi klonlayÄ±n:
```bash
git clone https://github.com/yourusername/aicrawler.git
cd aicrawler
```

2. Sanal ortam oluÅŸturun (Ã¶nerilen):
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate  # Windows
```

3. BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:
```bash
pip install -r requirements.txt
```

## ğŸ’» KullanÄ±m

### Streamlit Web ArayÃ¼zÃ¼

Streamlit arayÃ¼zÃ¼nÃ¼ baÅŸlatmak iÃ§in:

```bash
streamlit run app.py
```

TarayÄ±cÄ±nÄ±zda `http://localhost:8501` adresine gidin.

### Komut SatÄ±rÄ± Test Script

Test script'ini Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
python test_crawler.py
```

### Python API KullanÄ±mÄ±

```python
from src.core.workflow import CrawlerPipeline

# Pipeline oluÅŸtur
pipeline = CrawlerPipeline()

# Web sitesini tara
result = pipeline.crawl(
    start_url="https://example.com",
    max_depth=2,
    max_pages=10
)

# SonuÃ§larÄ± kullan
print(f"Taranan sayfa sayÄ±sÄ±: {result.total_crawled}")
for page in result.pages:
    print(f"URL: {page.url}")
    print(f"BaÅŸlÄ±k: {page.title}")
    print(f"Ä°Ã§erik: {page.cleaned_content[:200]}...")
```

## ğŸ—ï¸ Proje YapÄ±sÄ±

```
aicrawler/
â”œâ”€â”€ app.py                  # Streamlit web arayÃ¼zÃ¼
â”œâ”€â”€ test_crawler.py         # Test script
â”œâ”€â”€ requirements.txt        # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ .env                    # KonfigÃ¼rasyon dosyasÄ±
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ state.py       # State tanÄ±mlamalarÄ±
â”‚   â”‚   â”œâ”€â”€ crawler.py     # Web crawler modÃ¼lÃ¼
â”‚   â”‚   â”œâ”€â”€ cleaner.py     # HTML temizleme modÃ¼lÃ¼
â”‚   â”‚   â”œâ”€â”€ storage.py     # Veri depolama ve export
â”‚   â”‚   â””â”€â”€ workflow.py    # LangGraph workflow
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/             # YardÄ±mcÄ± fonksiyonlar
â”‚   â””â”€â”€ ui/                # UI bileÅŸenleri
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ exports/           # Export edilen dosyalar
â”‚
â”œâ”€â”€ docs/                  # DokÃ¼mantasyon
â””â”€â”€ tests/                 # Test dosyalarÄ±
```

## ğŸ”§ KonfigÃ¼rasyon

`.env` dosyasÄ±nda ayarlanabilir parametreler:

```env
DEFAULT_DEPTH=2            # VarsayÄ±lan tarama derinliÄŸi
DEFAULT_LIMIT=10           # VarsayÄ±lan sayfa limiti
EXPORT_DIR=data/exports    # Export dizini
USER_AGENT=...             # HTTP User-Agent
```

## ğŸ“Š LangGraph Workflow

AICrawler'Ä±n iÅŸ akÄ±ÅŸÄ± 3 ana node'dan oluÅŸur:

1. **Fetch Node**: Web sayfalarÄ±nÄ± indirir ve linklerini Ã§Ä±karÄ±r
2. **Clean Node**: HTML iÃ§eriÄŸini temizler ve anlamlÄ± metni Ã§Ä±karÄ±r
3. **Save Node**: TemizlenmiÅŸ veriyi JSON formatÄ±nda saklar

```
[Start] â†’ [Fetch] â†’ [Should Continue?] â†’ [Clean] â†’ [Save] â†’ [End]
             â†‘              â†“
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§¹ HTML Temizleme

Temizlenen elementler:
- Script ve style taglarÄ±
- Ä°frame, embed, object elementleri
- Form elementleri
- Reklam iÃ§erikleri (class/id pattern matching)
- BoÅŸ taglar
- HTML yorumlarÄ±

## ğŸ“¤ Export FormatlarÄ±

### JSON Format
```json
{
  "metadata": {
    "start_url": "https://example.com",
    "max_depth": 2,
    "max_pages": 10,
    "total_crawled": 8,
    "export_date": "2024-01-15T10:30:00"
  },
  "pages": [
    {
      "url": "https://example.com",
      "title": "Example Domain",
      "cleaned_content": "...",
      "metadata": {...}
    }
  ]
}
```

### Markdown Format
- BaÅŸlÄ±k ve metadata bilgileri
- Her sayfa iÃ§in ayrÄ± bÃ¶lÃ¼m
- TemizlenmiÅŸ iÃ§erik Ã¶nizlemesi
- Hata listesi

### PDF Format
- Profesyonel gÃ¶rÃ¼nÃ¼mlÃ¼ rapor
- Sayfa baÅŸlÄ±klarÄ± ve iÃ§erikleri
- Metadata ve istatistikler

## ğŸ§ª Test

Proje 3 farklÄ± test sitesi ile test edilmiÅŸtir:
1. quotes.toscrape.com - Basit yapÄ±lÄ± site
2. httpbin.org - HTTP test servisi
3. example.com - Minimal Ã¶rnek site

## ğŸ“ Notlar

- Crawler, robots.txt kurallarÄ±na saygÄ± gÃ¶stermez (demo amaÃ§lÄ±)
- Her istek arasÄ±nda 0.5 saniye bekleme sÃ¼resi vardÄ±r
- BÃ¼yÃ¼k siteler iÃ§in sayfa limitini kullanmanÄ±z Ã¶nerilir

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -m 'Add amazing feature'`)
4. Branch'inizi push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ‘¥ Ä°letiÅŸim

Proje DanÄ±ÅŸmanÄ±: Yunus Emre DEMÄ°RDAÄ

---

**Not**: Bu proje, LangGraph ile web crawling ve veri temizleme konularÄ±nda pratik kazanmak amacÄ±yla geliÅŸtirilmiÅŸ bir staj projesidir.