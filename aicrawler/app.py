"""
AICrawler - Streamlit Web Interface
"""
import streamlit as st
import sys
from pathlib import Path
import json
import time
from datetime import datetime
import logging

# Add src to path
sys.path.append(str(Path(__file__).parent))

from src.core.workflow import CrawlerPipeline
from src.core.storage import DataExporter
from src.core.state import CrawlerState

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="AICrawler - Web Ä°Ã§erik Toplama",
    page_icon="ğŸ•·ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .stApp {
        background-color: #f5f5f5;
    }
    .main-header {
        background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .info-box {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 5px;
    }
    .success-box {
        background-color: #e8f5e9;
        border-left: 4px solid #4caf50;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 5px;
    }
    .error-box {
        background-color: #ffebee;
        border-left: 4px solid #f44336;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 5px;
    }
    .metric-card {
        background-color: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'crawl_results' not in st.session_state:
    st.session_state.crawl_results = None
if 'is_crawling' not in st.session_state:
    st.session_state.is_crawling = False


def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ•·ï¸ AICrawler</h1>
        <p>LangGraph TabanlÄ± AkÄ±llÄ± Web Ä°Ã§erik Toplama ve Temizleme Sistemi</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Crawl AyarlarÄ±")
        
        # URL input
        url = st.text_input(
            "ğŸŒ BaÅŸlangÄ±Ã§ URL'i",
            placeholder="https://example.com",
            help="Taramaya baÅŸlanacak web sitesi URL'ini girin"
        )
        
        # Parameters
        col1, col2 = st.columns(2)
        with col1:
            depth = st.number_input(
                "ğŸ“Š Derinlik",
                min_value=0,
                max_value=5,
                value=2,
                help="KaÃ§ seviye derinliÄŸe kadar tarama yapÄ±lacaÄŸÄ±nÄ± belirler"
            )
        
        with col2:
            limit = st.number_input(
                "ğŸ“„ Sayfa Limiti",
                min_value=1,
                max_value=100,
                value=10,
                help="Maksimum kaÃ§ sayfa taranacaÄŸÄ±nÄ± belirler"
            )
        
        # Start button
        start_crawl = st.button(
            "ğŸš€ TaramayÄ± BaÅŸlat",
            type="primary",
            disabled=st.session_state.is_crawling or not url,
            use_container_width=True
        )
        
        st.divider()
        
        # Info
        st.info("""
        **KullanÄ±m TalimatlarÄ±:**
        1. Taramak istediÄŸiniz web sitesinin URL'ini girin
        2. Derinlik ve sayfa limiti ayarlayÄ±n
        3. "TaramayÄ± BaÅŸlat" butonuna tÄ±klayÄ±n
        4. SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼leyin ve dÄ±ÅŸa aktarÄ±n
        """)
    
    # Main content area
    if start_crawl and url:
        st.session_state.is_crawling = True
        
        # Create progress containers
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Crawl info
        with st.container():
            st.markdown('<div class="info-box">', unsafe_allow_html=True)
            st.write(f"**Taranan URL:** {url}")
            st.write(f"**Derinlik:** {depth} | **Sayfa Limiti:** {limit}")
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Execute crawl
        try:
            pipeline = CrawlerPipeline()
            
            # Show live updates
            status_text.text("ğŸ”„ Tarama baÅŸlatÄ±lÄ±yor...")
            progress_bar.progress(10)
            
            # Run the crawler
            with st.spinner("Sayfalar taranÄ±yor..."):
                result = pipeline.crawl(url, max_depth=depth, max_pages=limit)
            
            progress_bar.progress(100)
            status_text.text("âœ… Tarama tamamlandÄ±!")
            
            # Store results
            st.session_state.crawl_results = result
            st.session_state.is_crawling = False
            
            # Show success message
            st.markdown('<div class="success-box">', unsafe_allow_html=True)
            st.success(f"Tarama baÅŸarÄ±yla tamamlandÄ±! Toplam {result.total_crawled} sayfa iÅŸlendi.")
            st.markdown('</div>', unsafe_allow_html=True)
            
        except Exception as e:
            st.session_state.is_crawling = False
            st.markdown('<div class="error-box">', unsafe_allow_html=True)
            st.error(f"Tarama sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
            st.markdown('</div>', unsafe_allow_html=True)
            logger.error(f"Crawl error: {str(e)}")
    
    # Display results
    if st.session_state.crawl_results:
        result = st.session_state.crawl_results
        
        # Metrics
        st.subheader("ğŸ“Š Tarama Ä°statistikleri")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Taranan Sayfalar", result.total_crawled)
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col2:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Toplam BaÄŸlantÄ±", sum(len(p.links) for p in result.pages))
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col3:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("Hata SayÄ±sÄ±", len(result.errors))
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col4:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("BaÅŸarÄ± OranÄ±", f"{(result.total_crawled / (result.total_crawled + len(result.errors)) * 100):.1f}%")
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Results tabs
        st.subheader("ğŸ“„ Tarama SonuÃ§larÄ±")
        tabs = st.tabs(["ğŸ“ Ä°Ã§erik Ã–nizleme", "ğŸ”— Taranan URL'ler", "âš ï¸ Hatalar", "ğŸ’¾ DÄ±ÅŸa Aktarma"])
        
        # Content preview tab
        with tabs[0]:
            if result.pages:
                page_titles = [f"{i+1}. {p.title or p.url}" for i, p in enumerate(result.pages)]
                selected_page_idx = st.selectbox("Sayfa SeÃ§in", range(len(page_titles)), format_func=lambda x: page_titles[x])
                
                if selected_page_idx is not None:
                    page = result.pages[selected_page_idx]
                    
                    # Page info
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.write(f"**URL:** {page.url}")
                    with col2:
                        st.write(f"**Taranma ZamanÄ±:** {page.crawled_at.strftime('%H:%M:%S')}")
                    
                    # Content
                    if page.cleaned_content:
                        with st.expander("TemizlenmiÅŸ Ä°Ã§erik", expanded=True):
                            # Show first 1000 characters
                            content_preview = page.cleaned_content[:1000]
                            if len(page.cleaned_content) > 1000:
                                content_preview += "..."
                            st.text_area("", content_preview, height=300, disabled=True)
                    else:
                        st.warning("Bu sayfa iÃ§in temizlenmiÅŸ iÃ§erik bulunamadÄ±.")
            else:
                st.info("HenÃ¼z taranmÄ±ÅŸ sayfa bulunmuyor.")
        
        # URLs tab
        with tabs[1]:
            if result.pages:
                st.write(f"**Toplam {len(result.visited_urls)} URL tarandÄ±:**")
                
                # Create a dataframe for better display
                url_data = []
                for page in result.pages:
                    url_data.append({
                        "URL": page.url,
                        "BaÅŸlÄ±k": page.title or "BaÅŸlÄ±ksÄ±z",
                        "BaÄŸlantÄ± SayÄ±sÄ±": len(page.links),
                        "Ä°Ã§erik UzunluÄŸu": len(page.cleaned_content) if page.cleaned_content else 0
                    })
                
                st.dataframe(url_data, use_container_width=True)
            else:
                st.info("HenÃ¼z taranmÄ±ÅŸ URL bulunmuyor.")
        
        # Errors tab
        with tabs[2]:
            if result.errors:
                st.error(f"Toplam {len(result.errors)} hata oluÅŸtu:")
                for error in result.errors:
                    st.write(f"- **{error['url']}**: {error['error']}")
            else:
                st.success("Tarama sÄ±rasÄ±nda hata oluÅŸmadÄ±!")
        
        # Export tab
        with tabs[3]:
            st.write("Tarama sonuÃ§larÄ±nÄ± farklÄ± formatlarda dÄ±ÅŸa aktarabilirsiniz:")
            
            exporter = DataExporter()
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“„ JSON Ä°ndir", use_container_width=True):
                    json_path = exporter.export_to_json(result)
                    with open(json_path, 'r', encoding='utf-8') as f:
                        st.download_button(
                            label="ğŸ’¾ JSON DosyasÄ±nÄ± Ä°ndir",
                            data=f.read(),
                            file_name=Path(json_path).name,
                            mime="application/json"
                        )
            
            with col2:
                if st.button("ğŸ“ Markdown Ä°ndir", use_container_width=True):
                    md_path = exporter.export_to_markdown(result)
                    with open(md_path, 'r', encoding='utf-8') as f:
                        st.download_button(
                            label="ğŸ’¾ Markdown DosyasÄ±nÄ± Ä°ndir",
                            data=f.read(),
                            file_name=Path(md_path).name,
                            mime="text/markdown"
                        )
            
            with col3:
                if st.button("ğŸ“‘ PDF Ä°ndir", use_container_width=True):
                    pdf_path = exporter.export_to_pdf(result)
                    with open(pdf_path, 'rb') as f:
                        st.download_button(
                            label="ğŸ’¾ PDF DosyasÄ±nÄ± Ä°ndir",
                            data=f.read(),
                            file_name=Path(pdf_path).name,
                            mime="application/pdf"
                        )
            
            st.info("ğŸ’¡ Ä°pucu: Dosyalar aynÄ± zamanda 'data/exports' klasÃ¶rÃ¼ne de kaydedilir.")
    
    # Footer
    st.divider()
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 1rem;">
        <p>AICrawler v1.0 | LangGraph ile geliÅŸtirilmiÅŸtir | Â© 2024</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()